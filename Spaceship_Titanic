{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom tqdm.notebook import tqdm\ntorch.manual_seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:03.851883Z","iopub.execute_input":"2024-07-31T11:49:03.852232Z","iopub.status.idle":"2024-07-31T11:49:08.056085Z","shell.execute_reply.started":"2024-07-31T11:49:03.852202Z","shell.execute_reply":"2024-07-31T11:49:08.055187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.057951Z","iopub.execute_input":"2024-07-31T11:49:08.058331Z","iopub.status.idle":"2024-07-31T11:49:08.091982Z","shell.execute_reply.started":"2024-07-31T11:49:08.058306Z","shell.execute_reply":"2024-07-31T11:49:08.090967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Not using Transform in getitem, as data is very small we will store modified data in class object and will do all modification at once while initializing\n# Normally it is optimal and efficient to do these modifications while getting the items (in __getitem__) from the dataset - this maintain original data in object init and only \n# do operations only on calling the batch.\nclass spaceDataset(Dataset):\n    def __init__(self, csv_file, target_col = None, drop_col: list = None, split_col: str = None, dummy_col: list = None, norm_col: list = None, scaler = None, Device = 'cpu'):\n        self.dataframe = pd.read_csv(csv_file)\n        self.drop_col = drop_col\n        self.split_col = split_col\n        self.dummy_col = dummy_col\n        self.target_col = target_col\n        self.norm_col = norm_col\n        self.scaler  = scaler\n        self.Device = Device\n        \n        if self.drop_col:\n            self.dataframe.drop(columns=self.drop_col, inplace=True)\n            \n        if self.target_col:\n            self.dataframe.dropna(inplace=True)\n        else:\n            self.dataframe.fillna(method='ffill', inplace = True)  # Need to put some logic in this, can go with 0 for the base model as the test nan values count is very low.\n        \n        if self.split_col:\n            self.dataframe[[self.split_col+'_0', self.split_col+'_1', self.split_col+'_2']] = self.dataframe[self.split_col].str.split('/', expand=True)\n            self.dataframe.drop(columns=[self.split_col], inplace=True)\n        \n        if self.dummy_col:\n            self.dataframe = pd.get_dummies(self.dataframe, columns = self.dummy_col, drop_first = True)\n        self.dataframe = self.dataframe.astype('float32')\n        \n        if self.scaler and self.target_col:\n            self.scaler.fit(self.dataframe[self.norm_col])\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx: int):\n        row = self.dataframe.iloc[idx]\n        row[self.norm_col] = self.scaler.transform([row[self.norm_col]])[0]\n        if self.target_col:\n            target = row[self.target_col]\n            features = row.drop(self.target_col)\n            \n            # Convert features and target to tensors\n            features_tensor = torch.tensor(features.values, dtype=torch.float32, device = self.Device)\n            target_tensor = torch.tensor(target, dtype=torch.float32, device = self.Device)\n\n            return features_tensor, target_tensor\n        return torch.tensor(row.values, dtype = torch.float32, device = self.Device)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.093229Z","iopub.execute_input":"2024-07-31T11:49:08.093521Z","iopub.status.idle":"2024-07-31T11:49:08.109160Z","shell.execute_reply.started":"2024-07-31T11:49:08.093483Z","shell.execute_reply":"2024-07-31T11:49:08.108350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseClassifier(nn.Module):\n    def __init__(self, input_size = None):\n        super().__init__()\n        # Where we define all the parts of the model\n        self.linear_1 = nn.Linear(input_size, 42)\n        self.bnorm_1 = nn.BatchNorm1d(42)\n        self.relu_1 = nn.LeakyReLU(negative_slope=0.03)\n        self.linear_2 = nn.Linear(42, 84)\n        self.bnorm_2 = nn.BatchNorm1d(84)\n        self.relu_2 = nn.LeakyReLU(negative_slope=0.03)\n        self.linear_3 = nn.Linear(84, 128)\n        self.bnorm_3 = nn.BatchNorm1d(128)\n        self.relu_3 = nn.LeakyReLU(negative_slope=0.03)\n        self.linear_4 = nn.Linear(128, 1)\n    \n    def forward(self, x):\n        # Connect these parts and return the output\n        x = self.linear_1(x)\n        x = self.bnorm_1(x)\n        x = self.relu_1(x)\n        x = self.linear_2(x)\n        x = self.bnorm_2(x)\n        x = self.relu_2(x)\n        x = self.linear_3(x)\n        x = self.bnorm_3(x)\n        x = self.relu_3(x)\n        output = self.linear_4(x)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.110423Z","iopub.execute_input":"2024-07-31T11:49:08.110863Z","iopub.status.idle":"2024-07-31T11:49:08.124098Z","shell.execute_reply.started":"2024-07-31T11:49:08.110832Z","shell.execute_reply":"2024-07-31T11:49:08.123210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_file = '/kaggle/input/spaceship-titanic/train.csv'\ntarget_col = 'Transported'\ndrop_col = ['PassengerId', 'Name']\nsplit_col = 'Cabin'\ndummy_col = ['HomePlanet', 'Destination', split_col+'_0', split_col+'_2']\nnorm_col = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', split_col+'_1']\nscaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.126443Z","iopub.execute_input":"2024-07-31T11:49:08.126778Z","iopub.status.idle":"2024-07-31T11:49:08.134093Z","shell.execute_reply.started":"2024-07-31T11:49:08.126746Z","shell.execute_reply":"2024-07-31T11:49:08.133208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = spaceDataset(csv_file, target_col, drop_col, split_col, dummy_col, norm_col, scaler, Device)\n# Define split sizes\ntotal_size = len(dataset)\ntrain_size = int(0.9 * total_size)  # 90% for training\nval_size = total_size - train_size\n# Split the dataset\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.135165Z","iopub.execute_input":"2024-07-31T11:49:08.135412Z","iopub.status.idle":"2024-07-31T11:49:08.277658Z","shell.execute_reply.started":"2024-07-31T11:49:08.135391Z","shell.execute_reply":"2024-07-31T11:49:08.276895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features, labels in train_dataset:\n    print(features)\n    print(len(features))\n    print(features.device)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.278729Z","iopub.execute_input":"2024-07-31T11:49:08.279007Z","iopub.status.idle":"2024-07-31T11:49:08.714320Z","shell.execute_reply.started":"2024-07-31T11:49:08.278982Z","shell.execute_reply":"2024-07-31T11:49:08.713449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple training loop\nnum_epochs = 25\ntrain_losses, val_losses = [], []\n\nmodel = BaseClassifier(input_size = 21)\nmodel.to(Device)\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss(reduction = 'sum')\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    running_loss = 0.0\n    all_targets = []\n    all_preds = []\n    for features, labels in tqdm(train_loader, desc='Training loop'):\n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        \n        preds = (outputs > 0.5).float()\n        all_targets.extend(labels.to('cpu').numpy())\n        all_preds.extend(preds.squeeze().detach().to('cpu').numpy())\n        \n    train_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n    train_accuracy = accuracy_score(all_targets, all_preds)\n    \n    # Validation phase\n    model.eval()\n    running_loss = 0.0\n    all_targets = []\n    all_preds = []\n    with torch.inference_mode():\n        for features, labels in tqdm(val_loader, desc='Validation loop'):  \n            outputs = model(features)\n            loss = criterion(outputs.squeeze(), labels)\n            running_loss += loss.item()\n            \n            preds = (outputs > 0.5).float()\n            all_targets.extend(labels.to('cpu').numpy())\n            all_preds.extend(preds.squeeze().detach().to('cpu').numpy())\n            \n    val_loss = running_loss / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    test_accuracy = accuracy_score(all_targets, all_preds)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Train accuracy: {train_accuracy}, Validation loss: {val_loss}, Test accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:49:08.715706Z","iopub.execute_input":"2024-07-31T11:49:08.716248Z","iopub.status.idle":"2024-07-31T11:54:29.419169Z","shell.execute_reply.started":"2024-07-31T11:49:08.716216Z","shell.execute_reply":"2024-07-31T11:54:29.418215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = spaceDataset(csv_file = '/kaggle/input/spaceship-titanic/test.csv', target_col = None, drop_col = drop_col, split_col = split_col, dummy_col = dummy_col, norm_col = norm_col, scaler = scaler, Device = Device)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:54:29.420437Z","iopub.execute_input":"2024-07-31T11:54:29.420935Z","iopub.status.idle":"2024-07-31T11:54:29.479524Z","shell.execute_reply.started":"2024-07-31T11:54:29.420906Z","shell.execute_reply":"2024-07-31T11:54:29.478828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features in test_dataset:\n    print(features)\n    print(len(features))\n    print(features.device)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:54:29.480665Z","iopub.execute_input":"2024-07-31T11:54:29.481044Z","iopub.status.idle":"2024-07-31T11:54:29.490044Z","shell.execute_reply.started":"2024-07-31T11:54:29.481008Z","shell.execute_reply":"2024-07-31T11:54:29.489200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nall_preds = []\nwith torch.inference_mode():\n    for features in tqdm(test_loader, desc='Testing loop'):  \n        outputs = model(features)\n\n        preds = (outputs > 0.5).float()\n        all_preds.extend(preds.squeeze().detach().to('cpu').numpy())","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:54:29.491085Z","iopub.execute_input":"2024-07-31T11:54:29.491324Z","iopub.status.idle":"2024-07-31T11:54:35.717102Z","shell.execute_reply.started":"2024-07-31T11:54:29.491299Z","shell.execute_reply":"2024-07-31T11:54:35.716285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ntest_df['Transported'] = np.array(all_preds, dtype = bool)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:58:25.350444Z","iopub.execute_input":"2024-07-31T11:58:25.351164Z","iopub.status.idle":"2024-07-31T11:58:25.387402Z","shell.execute_reply.started":"2024-07-31T11:58:25.351133Z","shell.execute_reply":"2024-07-31T11:58:25.386550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:58:32.862558Z","iopub.execute_input":"2024-07-31T11:58:32.862918Z","iopub.status.idle":"2024-07-31T11:58:32.868637Z","shell.execute_reply.started":"2024-07-31T11:58:32.862889Z","shell.execute_reply":"2024-07-31T11:58:32.867807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['PassengerId','Transported']].to_csv('output_2.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:58:38.267226Z","iopub.execute_input":"2024-07-31T11:58:38.267593Z","iopub.status.idle":"2024-07-31T11:58:38.280477Z","shell.execute_reply.started":"2024-07-31T11:58:38.267563Z","shell.execute_reply":"2024-07-31T11:58:38.279683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = pd.read_csv('/kaggle/working/output_2.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:58:42.279726Z","iopub.execute_input":"2024-07-31T11:58:42.280050Z","iopub.status.idle":"2024-07-31T11:58:42.288992Z","shell.execute_reply.started":"2024-07-31T11:58:42.280025Z","shell.execute_reply":"2024-07-31T11:58:42.288042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:58:43.724252Z","iopub.execute_input":"2024-07-31T11:58:43.724612Z","iopub.status.idle":"2024-07-31T11:58:43.734025Z","shell.execute_reply.started":"2024-07-31T11:58:43.724584Z","shell.execute_reply":"2024-07-31T11:58:43.733020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-31T11:57:06.192259Z","iopub.execute_input":"2024-07-31T11:57:06.192868Z","iopub.status.idle":"2024-07-31T11:57:06.198418Z","shell.execute_reply.started":"2024-07-31T11:57:06.192836Z","shell.execute_reply":"2024-07-31T11:57:06.197555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}